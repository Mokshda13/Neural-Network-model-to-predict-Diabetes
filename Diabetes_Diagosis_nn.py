# -*- coding: utf-8 -*-
"""
Created on Tue Nov 12 06:31:08 2019

@author: hp
"""

# -*- coding: utf-8 -*-
"""
Created on Sat Nov  9 23:34:23 2019

@author: hp
"""
from numpy import exp, array, random, dot
import numpy as np


import csv

preg = []
glu = []
Blood = []
Skin_Thickness = []
Insulin = []
BMI = []
DPF = []
Age = []



with open(r'C:\Users\hp\Downloads\DiabetesPredictionData.csv') as csvDataFile:
    csvReader = csv.reader(csvDataFile)
    for row in csvReader:
        preg.append(int(row[0]))
        glu.append(int(row[1]))
        Blood.append(int(row[2]))
        Skin_Thickness.append(int(row[3]))
        Insulin.append(int(row[4]))
        BMI.append(float(row[5]))
        DPF.append(float(row[6]))
        Age.append(int(row[7]))
        


def nor_denor_preg(v):
    v=int(v)
    if(v>1):
        normal = (v-min(preg))/(max(preg)-min(preg))
        return normal
    else:
        denormal = v*(max(preg)-min(preg)) + min(preg)
        return denormal

def nor_denor_glu(v):
    v=int(v)
    if(v>1):
        normal = (v-min(glu))/(max(glu)-min(glu))
        return normal
    else:
        denormal = v*(max(glu)-min(glu)) + min(glu)
        return denormal
        
def nor_denor_bp(v):
    v=int(v)
    if(v>1):
        normal = (v-min(Blood))/(max(Blood)-min(Blood))
        return normal
    else:
        denormal = v*(max(Blood)-min(Blood)) + min(Blood)
        return denormal
        
def nor_denor_ST(v):
    v=int(v)
    if(v>1):
        normal = (v-min(Skin_Thickness))/(max(Skin_Thickness)-min(Skin_Thickness))
        return normal
    else:
        denormal = v*(max(Skin_Thickness)-min(Skin_Thickness)) + min(Skin_Thickness)
        return denormal
        
def nor_denor_Insulin(v):
    v=int(v)
    if(v>1):
        normal = (v-min(Insulin))/(max(Insulin)-min(Insulin))
        return normal
    else:
        denormal = v*(max(Insulin)-min(Insulin)) + min(Insulin)
        return denormal
        
def nor_denor_bmi(v):
    v=int(v)
    if(v>1):
        normal = (v-min(BMI))/(max(BMI)-min(BMI))
        return normal
    else:
        denormal = v*(max(BMI)-min(BMI)) + min(BMI)
        return denormal

def nor_denor_dpf(v):
    v=int(v)
    if(v>1):
        normal = (v-min(DPF))/(max(DPF)-min(DPF))
        return normal
    else:
        denormal = v*(max(DPF)-min(DPF)) + min(DPF)
        return denormal

def nor_denor_age(v):
    v=int(v)
    if(v>1):
        normal = (v-min(Age))/(max(Age)-min(Age))
        return normal
    else:
        denormal = v*(max(Age)-min(Age)) + min(Age)
        return denormal

for i in range (0, len(preg)):
    a = nor_denor_preg(preg[i])
    preg[i] = a
    
for i in range (0, len(glu)):
    a = nor_denor_glu(glu[i])
    glu[i] = a


for i in range (0, len(Blood)):
    a = nor_denor_bp(Blood[i])
    Blood[i] = a


for i in range (0, len(Skin_Thickness)):
    a = nor_denor_ST(Skin_Thickness[i])
    Skin_Thickness[i] = a


for i in range (0, len(Insulin)):
    a = nor_denor_Insulin(Insulin[i])
    Insulin[i] = a
    

for i in range (0, len(BMI)):
    a = nor_denor_bmi(BMI[i])
    BMI[i] = a
    

for i in range (0, len(DPF)):
    a = nor_denor_dpf(DPF[i])
    DPF[i] = a

for i in range (0, len(Age)):
    a = nor_denor_age(Age[i])
    Age[i] = a
    

input_arr=np.array([])
input_arr.astype(float)
input_arr = np.array([[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1],[1,1,1,1.5,1,1,1,1,1,1]])
out_arr=np.array([])
out_arr.astype(float)
out_arr=np.array([[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5]])
input_arr = np.delete(input_arr, 0, axis=0)
out_arr = np.delete(out_arr, 0, axis=0)

results = []
count = 0

with open(r'C:\Users\hp\Downloads\DiabetesPredictionData.csv') as csvfile:
    reader = csv.reader(csvfile, quoting=csv.QUOTE_NONNUMERIC) # change contents to floats
    for row in reader: # each row is a list
        results.append(row)
npa = np.asarray(results, dtype=np.float32)    
print(npa)         
count = 0  
temp_arr=[0.5]
with open(r'C:\Users\hp\Downloads\OutputN.csv') as csvDataFile:
    csvReader = csv.reader(csvDataFile)
    for row in csvReader:
        temp = int(row[0])
        temp_arr[0] = temp
        out_arr[count] = temp_arr
        count = count + 1




class NeuronLayer():
    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):
        self.synaptic_weights = 2 * random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1


class NeuralNetwork():
    def __init__(self, layer1, layer2):
        self.layer1 = layer1
        self.layer2 = layer2

    # The Sigmoid function, which describes an S shaped curve.
    # We pass the weighted sum of the inputs through this function to
    # normalise them between 0 and 1.
    def __sigmoid(self, x):
        return 1 / (1 + exp(-x))

    # The derivative of the Sigmoid function.
    # This is the gradient of the Sigmoid curve.
    # It indicates how confident we are about the existing weight.
    def __sigmoid_derivative(self, x):
        return x*(1 - x)

    # We train the neural network through a process of trial and error.
    # Adjusting the synaptic weights each time.
    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):
        for iteration in range(number_of_training_iterations):
            # Pass the training set through our neural network
            output_from_layer_1, output_from_layer_2 = self.think(training_set_inputs)

            # Calculate the error for layer 2 (The difference between the desired output
            # and the predicted output).
            layer2_error = training_set_outputs - output_from_layer_2
            layer2_delta = layer2_error * self.__sigmoid_derivative(output_from_layer_2)
            
            
            # Calculate the error for layer 1 (By looking at the weights in layer 1,
            # we can determine by how much layer 1 contributed to the error in layer 2).
            layer1_error = layer2_delta.dot(self.layer2.synaptic_weights.T)
            layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)
            
            # Calculate how much to adjust the weights by
            layer1_adjustment = training_set_inputs.T.dot(layer1_delta)
            layer2_adjustment = output_from_layer_1.T.dot(layer2_delta)

            # Adjust the weights.
            self.layer1.synaptic_weights += layer1_adjustment
            self.layer2.synaptic_weights += layer2_adjustment

    # The neural network thinks.
    def think(self, inputs):
        output_from_layer1 = self.__sigmoid(dot(inputs, self.layer1.synaptic_weights))
        output_from_layer2 = self.__sigmoid(dot(output_from_layer1, self.layer2.synaptic_weights))
        
        return output_from_layer1, output_from_layer2

    # The neural network prints its weights
    def print_weights(self):
        print( "    Layer 1(Hidden) : ")
        print(self.layer1.synaptic_weights)
        print( "    Layer 2 :")
        print(self.layer2.synaptic_weights)

if __name__ == "__main__":

    #Seed the random number generator
    random.seed(1)

    # Create layer 1 (6 neurons, each with 10 inputs)
    layer1 = NeuronLayer(6, 8)

    # Create layer 2 (a single neuron with 6 inputs)
    layer2 = NeuronLayer(1,6)

    # Combine the layers to create a neural network
    neural_network = NeuralNetwork(layer1, layer2)

    print("Stage 1) Random starting synaptic weights: ")
    neural_network.print_weights()
    training_set_inputs = npa
    training_set_outputs = out_arr
    
    # Train the neural network using the training set.
    # Do it 6,00,000 times and make small adjustments each time.
    neural_network.train(training_set_inputs, training_set_outputs, 60000)

    print ("Stage 2) New synaptic weights after training: ")
    neural_network.print_weights()
    
    
    # Test the neural network with a new situation.
    print( "Stage 3) Considering a new situation --- ")
    
    test = []
    with open(r'C:\Users\hp\Downloads\DiabetesPredictionData.csv') as csvfile:
        reader = csv.reader(csvfile, quoting=csv.QUOTE_NONNUMERIC) # change contents to floats
        for row in reader: # each row is a list
            test.append(row)
    input_test = np.asarray(test, dtype=np.float32)
    out_plot_arr=np.array([[.5],[.5],[.2],[.5],[.5],[.2],[.5],[.5],[.2],[.5],[.9]])
    
    
    hidden_state, output = neural_network.think(array(input_test))
    print(output)
    

   

